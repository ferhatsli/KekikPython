ğŸ•Š Bu dÃ¶kÃ¼man [**@KekikAkademi**](https://t.me/KekikAkademi "Telegram: @KekikAkademi") iÃ§in oluÅŸturulmuÅŸtur. âœŒğŸ¼
________________________________
# Python ile Veri KazÄ±ma
![Veri KazÄ±manÄ±n Ã‡alÄ±ÅŸma ÅemasÄ±](https://raw.githubusercontent.com/KekikAkademi/KekikPython/master/8-KekikDoviz/images/1-Veri-Kazimanin-Calisma-Semasi.jpeg)
Veri KazÄ±ma iki AÅŸamadan OluÅŸur;
 1. **Web Crawling**
 2. **Web Scraping**

## Veri KazÄ±ma Nedir?
Veri kazÄ±ma diÄŸer bir adÄ±yla Data Scraping, Web sitelerinden veri Ã§Ä±karma iÅŸlemidir.

### Neden Veri KazÄ±rÄ±z?
Web Ã¼zerinde birÃ§ok veri bulunuyor. Veri kazÄ±maâ€™da bu **veri toplama iÅŸlemini otomatikleÅŸtirir.** *DaÄŸÄ±tÄ±k halde olan verileri daha dÃ¼zgÃ¼n ÅŸekilde sunar.*
Ä°nternetâ€™ten kazÄ±nan verilerle birÃ§ok Ã§alÄ±ÅŸma yapÄ±labilir. BunlarÄ±n en yaygÄ±nlarÄ±ndan biri fiyat karÅŸÄ±laÅŸtÄ±rmasÄ±dÄ±r.

## Web Crawling Nedir?
**Web Crawling**, *HTTP baÄŸlantÄ±sÄ± ile herhangi bir site Ã¼zerinde gezinmek ve hedef site Ã¼zerinde yeralan linkleri toplamaktÄ±r.*
Bu linkler tÃ¼m site olabileceÄŸi gibi sÄ±nÄ±rlÄ± sayÄ±da da olabilir.
Ä°ÅŸte bu link toplama iÅŸlemini yerine getiren yapÄ±ya da _Web Crawler_ denilmektedir.
`Web Crawler`, `Web Spider`, `Robot` veya `Bot` hangi isimle hitap ederseniz; hepsi aynÄ± anlama gelmektedir ve yaptÄ±klarÄ± iÅŸ de aynÄ±dÄ±r.

### Ã‡alÄ±ÅŸma MantÄ±ÄŸÄ±
Web Crawlerâ€™larÄ±n Ã§alÄ±ÅŸma mantÄ±ÄŸÄ± karmaÅŸÄ±ktÄ±r ama iyi bir analiz ve sistemli Ã§alÄ±ÅŸma ile ortaya gÃ¼zel bir sonuÃ§ Ã§Ä±karÄ±labilir.

Ã–ncelikli olarak; veri almak istediÄŸiniz site veya platformu belirlemeniz gerekir.
Belirleme iÅŸleminden sonra hedefteki bu adresin iyi analiz edilmesi gerekir. Analizden kastÄ±m; verilerin bulunduÄŸu sayfalardÄ±r.
Bu sayfalarÄ±n kurulma ve dolaÅŸÄ±m ÅŸemasÄ± detaylÄ± olarak incelenmelidir. Ã–rneÄŸin; bazÄ± platformlar size istediÄŸiniz verileri hemen gÃ¶stermezler. Bir butona veya textâ€™e tÄ±klamanÄ±zÄ± veyahut bir ÅŸeyleri sÃ¼rÃ¼kleyip bÄ±rakmanÄ±zÄ± isteyebilirler.
BunlarÄ±n hepsi birer handikaptÄ±r ve Ã§Ã¶zÃ¼lmesi gereken birer sorundur.

_Web Crawler_â€™Ä±n temelde yapacaÄŸÄ± ÅŸey ÅŸudur; **belirlenen adresin tÃ¼m linklerini taramak ve listelemektir.** *Daha sonra da listelediÄŸi bu linklere sÄ±rasÄ±yla gider.*
Bu iÅŸlemi otomatize etmek de `Web Crawler` kavramÄ±nÄ± doÄŸurur.

## Web Scraping Nedir?
*Web Crawler*, betirlenen hedef adresteki linkâ€™leri tarar ve bu linleri bir liste ÅŸeklinde tutar. Yani bir link kuyruÄŸu oluÅŸturur. Bu oluÅŸturulan kuyruktaki linkâ€™lere sÄ±rasÄ±yla uÄŸrar ve yeni linkâ€™ler varsa onlarÄ± da listesine ekler â€“ tabiki listede olmayan linkleri-.
*Web crawler bir linkâ€™e uÄŸradÄ±ÄŸÄ± zaman* **devreye Web Scraping kavramÄ± girer.**
`Web Scraping`, linkâ€™teki belirtilen alanlarÄ±n toplanmasÄ± iÅŸlemidir. Yani bir nevi; veri toplama veya yÄ±ÄŸÄ±ndan veri Ã§Ä±karma olayÄ±dÄ±r.
`Web Crawler` ve `Web Scraping` *birbirleriyle partner olan kavramlar.* Yani birbirlerini *tamamlayÄ±cÄ± kavramlar* desek herhalde yanlÄ±ÅŸlÄ±k olmaz.

Åimdi Ã¶rnek bir senaryo oluÅŸturalÄ±m;
Elimizde Ã¶rnek bir x web sitesi olduÄŸunu varsayalÄ±m. Bu x web sitesinde bulunan ve reel kiÅŸilere ait olan; [Ad, Soyad, Telefon, Åehir] alanlarÄ±nÄ± almak istiyoruz.
YapacaÄŸÄ±mÄ±z iÅŸlem ÅŸÃ¶yle olacaktÄ±r;
Belirlenen adresteki linkler gezilecek (crawling),
SonrasÄ±nda da gezilen sayfalardaki veriler toplanacaktÄ±r (scraping).
Yani; istediÄŸimiz alanlarÄ± sayfadan koparÄ±p almamÄ±z gerekir..
Burdan sonrasÄ± da verilen dÃ¼zenli bir ÅŸekilde saklanmasÄ± iÅŸlemidir.

## Veri KazÄ±ma Stratejisi
Veri kazÄ±ma stratejisinin Ã§ok Ã¶nemli olduÄŸunu dÃ¼ÅŸÃ¼nÃ¼yorum, bir sitedeki bÃ¼tÃ¼n veriyi otomatik ÅŸekilde kazÄ±mak istiyorsak bir plana ihtiyacÄ±mÄ±z vardÄ±r.
Burada kÄ±saca veriyi hangi yollarla nasÄ±l bir Pipelineâ€™da kazÄ±yacaÄŸÄ±mÄ±zÄ± belirlemeliyiz.
![Veri KazÄ±ma Stratejisi](https://raw.githubusercontent.com/KekikAkademi/KekikPython/master/8-KekikDoviz/images/2-Veri-Kazima-Stratejisi.png)

## Nelere Dikkat edilmelidir?
Web Scraping yaparken dikkat edilmesi gereken en Ã¶nemli hususlar; doÄŸru bilgiyi almak ve bilgi bozulmasÄ±nÄ±n Ã¶nÃ¼ne geÃ§mek.

DoÄŸru bilgiyi almaktan kastÄ±m; iÅŸaretlenen alandaki veri alÄ±ndÄ±ktan sonra bu verinin temizlenmesi iÅŸlemidir. Bilgi genelde temiz gelir lakin bazen bazÄ± durumlarla karÅŸÄ±laÅŸabilirsiniz. Ã–rneÄŸin; bilgi iÃ§erisinde bir takÄ±m Ã¶zel iÅŸaretlemeler veya diÄŸer baÅŸka elementâ€™ler, JavaScript kodlarÄ± vs olabilir. Bu da demektir ki doÄŸru bilgiye henÃ¼z ulaÅŸamamÄ±ÅŸsÄ±nÄ±z. BÃ¶yle bir durumda alÄ±nan veriler, temiz olsalar bile temizleme yani sanitize iÅŸlemine tabi tutulmalarÄ± gerekir. Bu iÅŸlemi; web adresleri, mail adresleri veya telefon numaralarÄ± iÃ§in de kullanabilirsiniz, hatta kullanmalÄ±sÄ±nÄ±z da. Ã‡Ã¼nkÃ¼; web adresi yerine Ã¶rneÄŸin; www yazÄ±p bÄ±rakmÄ±ÅŸ olabilirler. DolayÄ±sÄ±yla doÄŸru bilgiye ulaÅŸmak iÃ§in; veriler temizlenmeli ve kalan deÄŸer varsa alÄ±nmalÄ±dÄ±r. Aksi halde kalan deÄŸer yoksa; zaten kimsenin iÅŸine yaramaz..!

Bilgi bozulmasÄ±nÄ±n Ã¶nÃ¼ne geÃ§mekten kastÄ±m ise; her sayfanÄ±n karakter kodlamasÄ± UTF-8 deÄŸildir. FarklÄ± karakter kodlamasÄ± kullanÄ±lmÄ±ÅŸ olabilir. DolayÄ±sÄ±yla siz bu deÄŸerlere ulaÅŸtÄ±ÄŸÄ±nÄ±z da muhtemelen bilgi bozulmasÄ±yla karÅŸÄ± karÅŸÄ±ya kalabilirsiniz. Bunun da Ã¶nÃ¼ne geÃ§mek iÃ§in; web sayfasÄ± scraping edilirken sayfanÄ±n karakter kodlamasÄ±nÄ± UTF-8 olarak veya ISO-8859- 1 olarak deÄŸiÅŸtirmenizdir. Aksi halde; aldÄ±ÄŸÄ±nÄ±z deÄŸerler bozulmuÅŸ yapÄ±da olacaktÄ±r. Bu bozulmuÅŸ deÄŸerleri daha sonra da dÃ¼zenleyebilirsiniz fakat bu oldukÃ§a zor ve meÅŸakkatli bir durum teÅŸkil edecektir. O yÃ¼zden hiÃ§ bu iÅŸe girmeden; scraping sÄ±rasÄ±nda bilgi bozulmasÄ±nÄ±n Ã¶nÃ¼ne geÃ§meliyiz.

## Verilerin SaklanmasÄ±
Crawling iÅŸleminden sonra; Scraping iÅŸlemini de yaptÄ±ÄŸÄ±mÄ±zÄ± varsayalÄ±m. Veriler elimize Ã¶bek Ã¶bek geliyor. Bu verilerin saklanmasÄ± sorunsalÄ± karÅŸÄ±mÄ±za Ã§Ä±kacaktÄ±r. Bu verilerin saklanmasÄ± iÃ§in; .json veya .txt dosyalarÄ±ndan medet ummayalÄ±m. BÃ¶yle bir dÃ¼ÅŸÃ¼nceniz varsa hemen aklÄ±nÄ±zdan Ã§Ä±karÄ±n..! Ã‡Ã¼nkÃ¼ sistemin yavaÅŸlamasÄ±na ve bir sÃ¼re sonra yanÄ±t verememesine neden olacaktÄ±r. Bunun yerine; MySQL tercih edilebilir bir seÃ§enek lakin SQLite tercih edilemez. SQLite hem veri saklama kapasitesi bakÄ±mÄ±ndan hem de MySQL seviyesinde bir pratikliÄŸe sahip olamamasÄ± bakÄ±mÄ±ndan tercih dÄ±ÅŸÄ±dÄ±r. Bana sorarsanÄ±z; MySQLâ€™den de ziyade eÅŸzamanlÄ± bir veri tabanÄ± kullanmanÄ±zÄ± size tavsiye ederim. Ã–rneÄŸin; Firebase gibi. Firebase yerine; NoSQL tabanlÄ± bir veri tabanÄ± da tercih edebilirsiniz. MySQL kullanmayÄ±n demiyorum, elbette MySQL tercih edilebilir bir seÃ§enektir diyorum.

Verilerin dÃ¼zgÃ¼n tasnif edilmesi, ilerleyen zamanlarda kullanÄ±m aÃ§Ä±sÄ±ndan oldukÃ§a Ã¶nemlidir. O yÃ¼zden alÄ±nan veriler; temiz ve ham veri olmalÄ±dÄ±r. Yani istenildiÄŸi zaman Ã¼zerinde iÅŸlem yapÄ±lmadan kullanÄ±labilinmelidir.

[Kaynak - 1](https://www.emrecanoztas.com/)
[Kaynak - 2](https://medium.com/kaveai)
________________________________
ğŸ“ƒ **Yandex.Disk BÃ¼nyemizde 900GB veri olmuÅŸtur.**

_PaylaÅŸÄ±lan KurslarÄ±n TÃ¼mÃ¼nÃ¼_ [**@KekikKahve**](https://t.me/KekikKahve) _Grubu notlarÄ±ndan Ã‡aÄŸÄ±rabilirsiniz.._

ğŸ•Šï¸ Bize **oy verip** _paylaÅŸarak_ destek olmaya ne dersin? âœŒğŸ¼
#
> Bu readme sayfasÄ± oluÅŸturulurken [prose.io](http://prose.io/ "prose.io") ve [stackedit.io](https://stackedit.io/app "stackedit.io") araÃ§larÄ±ndan yardÄ±m alÄ±nmÄ±ÅŸtÄ±r..
> Emojiler iÃ§in [webfx](https://www.webfx.com/tools/emoji-cheat-sheet/ "Emoji Cheat Sheet") sayfasÄ± kullanÄ±lmÄ±ÅŸtÄ±r.